{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lise Explorat√≥ria de Dados de da Rede Sonda üå¶Ô∏è\n",
    "\n",
    "## üìå Introdu√ß√£o\n",
    "Este notebook realiza uma an√°lise explorat√≥ria dos dados meteorol√≥gicos coletados de diversas esta√ß√µes. O objetivo √© entender a estrutura dos dados, avaliar sua qualidade e identificar padr√µes relevantes.\n",
    "\n",
    "## üìÇ Fonte dos Dados\n",
    "- Arquivos CSV formatados armazenados no ftp\n",
    "- Cont√™m medi√ß√µes de vari√°veis meteorol√≥gicas, solarim√©tricas e cameras.\n",
    "\n",
    "## üîç Objetivos da An√°lise\n",
    "1. **Carregar e explorar os dados**: verificar onde os dados est√£o armazenados, seu formato e estrutura.\n",
    "2. **Dimensionamento e vari√°veis dispon√≠veis**: entender o tamanho dos arquivos, n√∫mero de registros e colunas.\n",
    "3. **An√°lise temporal dos dados dispon√≠veis**: identificar o per√≠odo coberto e eventuais lacunas temporais.\n",
    "4. **Visualiza√ß√£o da distribui√ß√£o espacial das esta√ß√µes**: verificar a abrang√™ncia geogr√°fica das medi√ß√µes.\n",
    "5. **Explora√ß√£o inicial de distribui√ß√µes**: histogramas e estat√≠sticas b√°sicas das vari√°veis.\n",
    "6. **An√°lise de qualidade dos dados** *(√∫ltima etapa)*: identificar valores ausentes, inconsist√™ncias e flags de qualidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carregar e Explorar os Dados\n",
    "Vamos come√ßar listando o tamanho da base de dados que est√£o no diret√≥rio do ftp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diret√≥rio onde os arquivos est√£o localizados\n",
    "DIRETORIO = '../sonda/dados_formatados/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11G\t../sonda/dados_formatados/\n",
      "1,2G\t../sonda/dados_formatados/BRB\n",
      "1011M\t../sonda/dados_formatados/PTR\n",
      "989M\t../sonda/dados_formatados/FLN\n",
      "959M\t../sonda/dados_formatados/PMA\n",
      "759M\t../sonda/dados_formatados/JOI\n",
      "722M\t../sonda/dados_formatados/CPA\n",
      "714M\t../sonda/dados_formatados/SMS\n",
      "686M\t../sonda/dados_formatados/SLZ\n",
      "613M\t../sonda/dados_formatados/NAT\n",
      "536M\t../sonda/dados_formatados/CGR\n",
      "464M\t../sonda/dados_formatados/SBR\n",
      "413M\t../sonda/dados_formatados/TMA\n",
      "365M\t../sonda/dados_formatados/MCL\n",
      "349M\t../sonda/dados_formatados/ORN\n",
      "300M\t../sonda/dados_formatados/UBE\n",
      "284M\t../sonda/dados_formatados/BJL\n",
      "175M\t../sonda/dados_formatados/TLG\n",
      "174M\t../sonda/dados_formatados/CAI\n",
      "171M\t../sonda/dados_formatados/CTB\n",
      "55M\t../sonda/dados_formatados/CBA\n",
      "196K\t../sonda/dados_formatados/TRI\n",
      "196K\t../sonda/dados_formatados/SPK\n",
      "196K\t../sonda/dados_formatados/SCR\n",
      "196K\t../sonda/dados_formatados/RLM\n",
      "196K\t../sonda/dados_formatados/OPO\n",
      "196K\t../sonda/dados_formatados/MDS\n",
      "196K\t../sonda/dados_formatados/LEB\n",
      "196K\t../sonda/dados_formatados/CTS\n",
      "196K\t../sonda/dados_formatados/CPN\n",
      "196K\t../sonda/dados_formatados/CMS\n",
      "196K\t../sonda/dados_formatados/CHP\n",
      "196K\t../sonda/dados_formatados/BAB\n"
     ]
    }
   ],
   "source": [
    "# Exibe o tamanho de cada arquivo no diret√≥rio ordenado por tamanho de forma decrescente\n",
    "!du -h --max-depth=1 {DIRETORIO} | sort -rh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 3 tipos de dados:\n",
    "- Dados Meteorol√≥gicos\n",
    "- Dados Solarim√©tricos\n",
    "- Dados Anemometricos\n",
    "\n",
    "Abaixo vamos adicionar cada tipo de dado em uma lista para facilitar a an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# listar todos os dados Meteorol√≥gicos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_metereologicos = glob.glob(DIRETORIO + \"*/Meteorologicos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_metereologicos = [arquivo for arquivo in dados_metereologicos if 'YYYY_MM' not in arquivo]\n",
    "\n",
    "# listar todos os dados de Solarim√©tricos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_solarimetricos = glob.glob(DIRETORIO + \"*/Solarimetricos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_solarimetricos = [arquivo for arquivo in dados_solarimetricos if 'YYYY_MM' not in arquivo]\n",
    "\n",
    "# listar todos os dados de Anemometricos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_anemometricos = glob.glob(DIRETORIO + \"*/Anemometricos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_anemometricos = [arquivo for arquivo in dados_anemometricos if 'YYYY_MM' not in arquivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de arquivos Meteorologicos: 1036\n",
      "Quantidade de arquivos Solarimetricos: 1022\n",
      "Quantidade de arquivos Anemometricos: 0\n"
     ]
    }
   ],
   "source": [
    "# Listar a quantidade de arquivos em cada categoria\n",
    "print(f\"Quantidade de arquivos Meteorologicos: {len(dados_metereologicos)}\")\n",
    "print(f\"Quantidade de arquivos Solarimetricos: {len(dados_solarimetricos)}\")\n",
    "print(f\"Quantidade de arquivos Anemometricos: {len(dados_anemometricos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos a biblioteca duckdb para realizar a an√°lise dos dados. DuckDB funciona como um banco de dados SQL, mas em mem√≥ria, o que facilita a an√°lise de grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar a biblioteca DuckDB para manipula√ß√£o de dados\n",
    "import duckdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# Conectar ao banco de dados DuckDB e controla numero de threads\n",
    "con = duckdb.connect(database=':memory:', read_only=False, config={'threads': 4})\n",
    "\n",
    "# Remove qualquer arquivo tempor√°rio que possa existir\n",
    "!rm -rf .tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar a tabela no banco de dados\n",
    "def criar_base(con, dados, base, arquivo):\n",
    "    # Verifica se o arquivo j√° existe\n",
    "    if os.path.exists(arquivo):\n",
    "        con.execute(f\"CREATE TABLE {base} AS SELECT * FROM read_parquet('{arquivo}')\")\n",
    "    else:\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            CREATE TABLE {base} AS \n",
    "            SELECT * FROM read_csv_auto('{dados[0]}',\n",
    "                                        skip=2, \n",
    "                                        union_by_name=True, \n",
    "                                        all_varchar=True) \n",
    "            WHERE 1=0\n",
    "            \"\"\"\n",
    "            con.execute(query)\n",
    "            print(f\"Tabela {base} criada com sucesso\")\n",
    "        except:\n",
    "            print(f\"Erro ao criar tabela {base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apontar o caminho das bases de dados\n",
    "ARQV_METEOROLOGICO = '../sonda/dados_meteorologicos.parquet'\n",
    "ARQV_SOLARIMETRICA = '../sonda/dados_solarimetricos.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a base de dados meteorol√≥gicos\n",
    "criar_base(con, dados_metereologicos, 'base_meteorologica', ARQV_METEOROLOGICO)\n",
    "# Criar a base de dados solarim√©tricos\n",
    "criar_base(con, dados_solarimetricos, 'base_solarimetrica', ARQV_SOLARIMETRICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>min</th>\n",
       "      <th>tp_sfc</th>\n",
       "      <th>humid_sfc</th>\n",
       "      <th>press</th>\n",
       "      <th>rain</th>\n",
       "      <th>ws10_avg</th>\n",
       "      <th>ws10_std</th>\n",
       "      <th>wd10_avg</th>\n",
       "      <th>wd10_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:00:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:10:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>10</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:20:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>20</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:30:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>30</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:40:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>40</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "      <td>-5555.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym           timestamp  year  day  min  tp_sfc  humid_sfc   press  \\\n",
       "0     BJL 2014-06-01 00:00:00  2014  152    0  3333.0     3333.0  3333.0   \n",
       "1     BJL 2014-06-01 00:10:00  2014  152   10  3333.0     3333.0  3333.0   \n",
       "2     BJL 2014-06-01 00:20:00  2014  152   20  3333.0     3333.0  3333.0   \n",
       "3     BJL 2014-06-01 00:30:00  2014  152   30  3333.0     3333.0  3333.0   \n",
       "4     BJL 2014-06-01 00:40:00  2014  152   40  3333.0     3333.0  3333.0   \n",
       "\n",
       "     rain  ws10_avg  ws10_std  wd10_avg  wd10_std  \n",
       "0 -5555.0   -5555.0   -5555.0   -5555.0   -5555.0  \n",
       "1 -5555.0   -5555.0   -5555.0   -5555.0   -5555.0  \n",
       "2 -5555.0   -5555.0   -5555.0   -5555.0   -5555.0  \n",
       "3 -5555.0   -5555.0   -5555.0   -5555.0   -5555.0  \n",
       "4 -5555.0   -5555.0   -5555.0   -5555.0   -5555.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica o conteudo da tabela base_meteorologica\n",
    "con.execute(\"SELECT * FROM base_meteorologica LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acronym</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>min</th>\n",
       "      <th>glo_avg</th>\n",
       "      <th>glo_std</th>\n",
       "      <th>glo_max</th>\n",
       "      <th>glo_min</th>\n",
       "      <th>dif_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>dir_min</th>\n",
       "      <th>lw_avg</th>\n",
       "      <th>lw_std</th>\n",
       "      <th>lw_max</th>\n",
       "      <th>lw_min</th>\n",
       "      <th>temp_glo</th>\n",
       "      <th>temp_dir</th>\n",
       "      <th>temp_dif</th>\n",
       "      <th>temp_dome</th>\n",
       "      <th>temp_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:00:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:01:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:02:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:03:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>3</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:04:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>4</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  acronym           timestamp  year  day  min  glo_avg  glo_std  glo_max  \\\n",
       "0     BJL 2014-06-01 00:00:00  2014  152    0   3333.0   3333.0   3333.0   \n",
       "1     BJL 2014-06-01 00:01:00  2014  152    1   3333.0   3333.0   3333.0   \n",
       "2     BJL 2014-06-01 00:02:00  2014  152    2   3333.0   3333.0   3333.0   \n",
       "3     BJL 2014-06-01 00:03:00  2014  152    3   3333.0   3333.0   3333.0   \n",
       "4     BJL 2014-06-01 00:04:00  2014  152    4   3333.0   3333.0   3333.0   \n",
       "\n",
       "   glo_min  dif_avg  ...  dir_min  lw_avg  lw_std  lw_max  lw_min  temp_glo  \\\n",
       "0   3333.0   3333.0  ...   3333.0  3333.0  3333.0  3333.0  3333.0       0.0   \n",
       "1   3333.0   3333.0  ...   3333.0  3333.0  3333.0  3333.0  3333.0       0.0   \n",
       "2   3333.0   3333.0  ...   3333.0  3333.0  3333.0  3333.0  3333.0       0.0   \n",
       "3   3333.0   3333.0  ...   3333.0  3333.0  3333.0  3333.0  3333.0       0.0   \n",
       "4   3333.0   3333.0  ...   3333.0  3333.0  3333.0  3333.0  3333.0       0.0   \n",
       "\n",
       "   temp_dir  temp_dif  temp_dome  temp_case  \n",
       "0       0.0       0.0        0.0        0.0  \n",
       "1       0.0       0.0        0.0        0.0  \n",
       "2       0.0       0.0        0.0        0.0  \n",
       "3       0.0       0.0        0.0        0.0  \n",
       "4       0.0       0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica o conteudo da tabela base_solarimetrica\n",
    "con.execute(\"SELECT * FROM base_solarimetrica LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para popular a base de dados\n",
    "def inserir_dados(base, arquivo, con, periodico=True):\n",
    "    # Lendo os novos dados do CSV, pulando a segunda linha mas mantendo o cabe√ßalho\n",
    "    new_data = pd.read_csv(arquivo, skiprows=[1])\n",
    "    # Formata os dados for√ßando para que sejam n√∫mericos, e caso n√£o seja, substitui pelo valor original\n",
    "    new_data = new_data.apply(pd.to_numeric, errors='coerce').fillna(new_data)\n",
    "    # Converte a coluna de timestamp para datetime\n",
    "    new_data['timestamp'] = pd.to_datetime(new_data['timestamp'], errors='coerce')\n",
    "    # Pega o nome da esta√ß√£o do arquivo\n",
    "    estacao = new_data['acronym'][0]\n",
    "    # Pega o per√≠odo de tempo do arquivo\n",
    "    tempo = new_data['timestamp'].min(), new_data['timestamp'].max()\n",
    "    # Coluna de vari√°veis\n",
    "    variaveis = new_data.columns[5:]\n",
    "    # Para cada vari√°vel\n",
    "    for variavel in variaveis:\n",
    "        if periodico:\n",
    "            # Query periodica # Insere todos os dados de uma vez verificando se j√° existe\n",
    "            query = f\"SELECT COUNT(*) FROM {base} WHERE acronym = '{estacao}' AND timestamp >= '{tempo[0]}' AND timestamp <= '{tempo[1]}' AND {variavel} IS NOT NULL\"\n",
    "            if con.execute(query).fetchone()[0] > 0:\n",
    "                print(f\"J√° existe dados para a esta√ß√£o {estacao}, no per√≠odo de {tempo[0]} at√© {tempo[1]} para a vari√°vel {variavel}\")\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    con.execute(f\"INSERT INTO {base} VALUES ({', '.join([f'\"{estacao}\"'] + [f\"'{row}'\" for row in new_data[variavel]])})\")\n",
    "                except Exception as e:\n",
    "                    print('Erro ao inserir dados do arquivo', arquivo, 'na base', base, 'para a vari√°vel', variavel)\n",
    "                    print(e)\n",
    "        else: # N√£o periodico # Insere um a um verificando se j√° existe\n",
    "            for _, row in new_data[['timestamp', variavel]].iterrows():\n",
    "                query = f\"SELECT COUNT(*) FROM {base} WHERE acronym = '{estacao}' AND timestamp = '{row['timestamp']}' AND {variavel} IS NOT NULL\"\n",
    "                if con.execute(query).fetchone()[0] > 0:\n",
    "                    print(f\"J√° existe dados para a esta√ß√£o {estacao}, no per√≠odo de {row['timestamp']} para a vari√°vel {variavel}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        con.execute(f\"INSERT INTO {base} VALUES ({', '.join([f'\"{estacao}\"'] + [f\"'{row[col]}'\" for col in new_data.columns[1:]])})\")\n",
    "                    except Exception as e:\n",
    "                        print('Erro ao inserir dados do arquivo', arquivo, 'na base', base, 'para a vari√°vel', variavel)\n",
    "                        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inserir_dados_paralelo(base, arquivos, con, periodico=True):\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(inserir_dados, base, arquivo, con, periodico) for arquivo in arquivos]\n",
    "#         concurrent.futures.wait(futures)\n",
    "\n",
    "def inserir_dados_paralelo(base, arquivos, con, periodico=True):\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(inserir_dados, base, arquivo, con, periodico) for arquivo in arquivos]\n",
    "        concurrent.futures.wait(futures)    \n",
    "        \n",
    "def inserir_dados_sequencial(base, arquivos, con, periodico=True):\n",
    "    print(f\"Inserindo dados na base {base} de forma sequencial\")\n",
    "    for arquivo in arquivos:\n",
    "        inserir_dados(base, arquivo, con, periodico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir os dados meteorol√≥gicos\n",
    "inserir_dados_paralelo('base_meteorologica', dados_metereologicos[0:10], con, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserindo dados na base base_meteorologica de forma sequencial\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-03-01 00:00:00 at√© 2019-03-31 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-01-01 00:00:00 at√© 2019-01-31 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-02-01 00:00:00 at√© 2019-02-28 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2019-04-01 00:00:00 at√© 2019-04-30 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-07-01 00:00:00 at√© 2011-07-31 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-09-01 00:00:00 at√© 2011-09-30 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-02-01 00:00:00 at√© 2011-02-28 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-11-01 00:00:00 at√© 2011-11-30 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-01-01 00:00:00 at√© 2011-01-31 23:50:00 para a vari√°vel wd10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel tp_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel humid_sfc\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel press\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel rain\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel ws10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel ws10_std\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel wd10_avg\n",
      "J√° existe dados para a esta√ß√£o FLN, no per√≠odo de 2011-08-01 00:00:00 at√© 2011-08-31 23:50:00 para a vari√°vel wd10_std\n"
     ]
    }
   ],
   "source": [
    "inserir_dados_sequencial('base_meteorologica', dados_metereologicos[0:10], con, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular a base de dados solarim√©tricos\n",
    "for arquivo in dados_solarimetricos:\n",
    "    inserir_dados('base_solarimetrica', arquivo, con, periodico=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para criar a tabela no banco de dados\n",
    "def criar_tabela(con, dados, base, saida):\n",
    "    # Deleta tabela caso exista\n",
    "    con.execute(f\"DROP TABLE IF EXISTS {base}\")\n",
    "\n",
    "    # Verifica se o arquivo existe, caso exista leia o arquivo usando duckdb\n",
    "    if  os.path.exists(saida):\n",
    "        # Ler o arquivo parquet parquet usando duckdb\n",
    "        con.execute(f\"CREATE TABLE {base} AS SELECT * FROM read_parquet('{saida}')\")\n",
    "    else:\n",
    "        # Criar a tabela tempor√°ria vazia com base no primeiro arquivo CSV\n",
    "        query = f\"\"\"\n",
    "        CREATE TABLE {base} AS \n",
    "        SELECT * FROM read_csv_auto('{dados[0]}', \n",
    "                                    skip=2, \n",
    "                                    union_by_name=True, \n",
    "                                    all_varchar=True) \n",
    "        WHERE 1=0\n",
    "        \"\"\"\n",
    "        con.execute(query)  # Cria a tabela vazia com o esquema correto\n",
    "        # Inserir os dados em lote sem carregar tudo na mem√≥ria\n",
    "        for arquivo in sorted(dados):\n",
    "            print(f\"Inserindo dados do arquivo: {arquivo}\", end=\"\\r\", flush=True)\n",
    "            try:\n",
    "                query = f\"\"\"\n",
    "                INSERT INTO {base} \n",
    "                SELECT * FROM read_csv_auto('{arquivo}', skip=2, union_by_name=True, all_varchar=True)\n",
    "                \"\"\"\n",
    "                con.execute(query)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao processar o arquivo: {arquivo}\")\n",
    "                print(f\"   ‚û°Ô∏è Motivo: {e}\")\n",
    "\n",
    "        # Ler a primeira linha do arquivo CSV para obter os nomes das colunas\n",
    "        header = pd.read_csv(dados[0], nrows=1)\n",
    "        column_names = header.columns.tolist()  # Nomes das colunas\n",
    "\n",
    "        # Renomear as colunas com base nos nomes do arquivo CSV\n",
    "        for i, col in enumerate(column_names):\n",
    "            # Formata o nome da coluna conforme a nomenclatura do DuckDB (column01, column02, ...)\n",
    "            column_name = f\"column{str(i).zfill(2)}\"  # Preenche com zero √† esquerda para 2 d√≠gitos\n",
    "            # Renomeia a coluna pelo nome correto\n",
    "            con.execute(f\"ALTER TABLE {base} RENAME COLUMN {column_name} TO {col}\")\n",
    "\n",
    "        # Set column timestamp as datetime\n",
    "        con.execute(f\"ALTER TABLE {base} ALTER COLUMN timestamp SET DATA TYPE TIMESTAMP\")\n",
    "\n",
    "        # Seta coluna 1 como STRING\n",
    "        con.execute(f\"ALTER TABLE {base} ALTER COLUMN {column_names[0]} SET DATA TYPE STRING\")\n",
    "\n",
    "        # Todas as demais colunas como FLOAT a partir da segunda coluna\n",
    "        for i, col in enumerate(column_names[5:], start=5):\n",
    "            # Formata o dado da coluna caso exista valores com , virgula, substitui por .\n",
    "            con.execute(f\"UPDATE {base} SET {col} = REPLACE({col}, ',', '.')\")\n",
    "\n",
    "            # Formata o dado da coluna caso exista valores com - virgula, substitui por .\n",
    "            con.execute(f\"UPDATE {base} SET {col} = REPLACE({col}, '-', '0')\")\n",
    "\n",
    "            try:\n",
    "                con.execute(f\"ALTER TABLE {base} ALTER COLUMN {col} SET DATA TYPE FLOAT\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao processar a coluna: {col}\")\n",
    "                print(f\"   ‚û°Ô∏è Motivo: {e}\")\n",
    "\n",
    "        # Seta colunas 2, 3 e 4 como INTEGER\n",
    "        for i in column_names[2:5]:\n",
    "            try:\n",
    "                con.execute(f\"ALTER TABLE {base} ALTER COLUMN {i} SET DATA TYPE INTEGER\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao processar a coluna: {i}\")\n",
    "                print(f\"   ‚û°Ô∏è Motivo: {e}\")\n",
    "        \n",
    "        # Salvar os dados em Parquet na BASE\n",
    "        con.execute(f\"COPY {base} TO '{saida}' (FORMAT 'parquet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apontar o caminho das bases de dados\n",
    "BASE_METEOROLOGICA = '../sonda/dados_meteorologicos.parquet'\n",
    "BASE_SOLARIMETRICA = '../sonda/dados_solarimetricos.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a tabela meteorol√≥gica\n",
    "tabela_meteorologica = 'base_meteorologica'\n",
    "# criar_tabela(con, dados_metereologicos, tabela_meteorologica, BASE_METEOROLOGICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a tabela solarim√©trica\n",
    "tabela_solarimetrica = 'base_solarimetrica'\n",
    "# criar_tabela(con, dados_solarimetricos, tabela_solarimetrica, BASE_SOLARIMETRICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as bases de dados criadas e carregadas em mem√≥ria, podemos come√ßar a an√°lise explorat√≥ria dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir as primeiras linhas da tabela meteorological\n",
    "con.execute(f\"SELECT * FROM {tabela_meteorologica} LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir as primeiras linhas da tabela solarimetrica\n",
    "tabela_solarimetrica = 'base_solarimetrica'\n",
    "con.execute(f\"SELECT * FROM {tabela_solarimetrica} LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega apenas colunas de dados meteorol√≥gicos\n",
    "colunas_meteoro = con.execute(f\"SELECT * FROM {tabela_meteorologica} LIMIT 1\").description\n",
    "colunas_meteoro = [c[0] for c in colunas_meteoro[5:]]\n",
    "print(f\"Colunas de dados meteorol√≥gicos: {colunas_meteoro}\")\n",
    "\n",
    "# Pega apenas colunas de dados solarim√©tricos\n",
    "colunas_solar = con.execute(f\"SELECT * FROM {tabela_solarimetrica} LIMIT 1\").description\n",
    "colunas_solar = [c[0] for c in colunas_solar[5:]]\n",
    "print(f\"Colunas de dados solarim√©tricos: {colunas_solar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira an√°lise ser√° a verifica√ß√£o temporal dos dados, para entender o per√≠odo coberto e eventuais lacunas temporais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_temporal(con, base):\n",
    "    # Faz um agrupamento por acronym e timestamp para verificar se os dados s√£o temporais, fazendo a contagem de registros por dia\n",
    "    query = f\"\"\"\n",
    "    SELECT acronym, DATE_TRUNC('day', timestamp) AS data, COUNT(*) AS registros\n",
    "    FROM {base}\n",
    "    GROUP BY acronym, data\n",
    "    ORDER BY data\n",
    "    \"\"\"\n",
    "    return con.execute(query).fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se os dados meteorol√≥gicos s√£o temporais\n",
    "verifica_temporal(con, tabela_meteorologica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_dados_invalidos(con, base, colunas): \n",
    "    query = f\"\"\"\n",
    "    SELECT acronym, COUNT(*) AS total_dados,\n",
    "    \"\"\"\n",
    "    # Para cada coluna em colunas_meteoro, criamos a parte do \"dados_invalidos\"\n",
    "    for i, coluna in enumerate(colunas):\n",
    "        query += f\"\"\"\n",
    "        SUM(CASE WHEN \\\"{coluna}\\\" = 3333.0 THEN 1 ELSE 0 END) AS {coluna}_3333,\n",
    "        SUM(CASE WHEN \\\"{coluna}\\\" = -5555.0 THEN 1 ELSE 0 END) AS {coluna}_minus_5555,\n",
    "        \"\"\"\n",
    "    # Remover a √∫ltima v√≠rgula da consulta\n",
    "    query = query.rstrip(\",\\n\")\n",
    "    # Adiciona a parte do FROM e GROUP BY\n",
    "    query += f\"\"\"\n",
    "    FROM \\\"{base}\\\"\n",
    "    GROUP BY acronym\n",
    "    \"\"\"\n",
    "    # Executar a query\n",
    "    df = con.execute(query).fetch_df()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verificar_dados_invalidos(con, tabela_meteorologica, colunas_meteoro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verificar_dados_invalidos(con, tabela_solarimetrica, colunas_solar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
