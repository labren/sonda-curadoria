{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lise Explorat√≥ria de Dados de da Rede Sonda üå¶Ô∏è\n",
    "\n",
    "## üìå Introdu√ß√£o\n",
    "Este notebook realiza uma an√°lise explorat√≥ria dos dados meteorol√≥gicos coletados de diversas esta√ß√µes. O objetivo √© entender a estrutura dos dados, avaliar sua qualidade e identificar padr√µes relevantes.\n",
    "\n",
    "## üìÇ Fonte dos Dados\n",
    "- Arquivos CSV formatados armazenados no ftp\n",
    "- Cont√™m medi√ß√µes de vari√°veis meteorol√≥gicas, solarim√©tricas e cameras.\n",
    "\n",
    "## üîç Objetivos da An√°lise\n",
    "1. **Carregar e explorar os dados**: verificar onde os dados est√£o armazenados, seu formato e estrutura.\n",
    "2. **Dimensionamento e vari√°veis dispon√≠veis**: entender o tamanho dos arquivos, n√∫mero de registros e colunas.\n",
    "3. **An√°lise temporal dos dados dispon√≠veis**: identificar o per√≠odo coberto e eventuais lacunas temporais.\n",
    "4. **Visualiza√ß√£o da distribui√ß√£o espacial das esta√ß√µes**: verificar a abrang√™ncia geogr√°fica das medi√ß√µes.\n",
    "5. **Explora√ß√£o inicial de distribui√ß√µes**: histogramas e estat√≠sticas b√°sicas das vari√°veis.\n",
    "6. **An√°lise de qualidade dos dados** *(√∫ltima etapa)*: identificar valores ausentes, inconsist√™ncias e flags de qualidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carregar e Explorar os Dados\n",
    "Vamos come√ßar listando o tamanho da base de dados que est√£o no diret√≥rio do ftp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diret√≥rio onde os arquivos est√£o localizados\n",
    "DIRETORIO = '../sonda/dados_formatados/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11G\t../sonda/dados_formatados/\n",
      "1,2G\t../sonda/dados_formatados/BRB\n",
      "1011M\t../sonda/dados_formatados/PTR\n",
      "989M\t../sonda/dados_formatados/FLN\n",
      "959M\t../sonda/dados_formatados/PMA\n",
      "759M\t../sonda/dados_formatados/JOI\n",
      "722M\t../sonda/dados_formatados/CPA\n",
      "714M\t../sonda/dados_formatados/SMS\n",
      "686M\t../sonda/dados_formatados/SLZ\n",
      "613M\t../sonda/dados_formatados/NAT\n",
      "536M\t../sonda/dados_formatados/CGR\n",
      "464M\t../sonda/dados_formatados/SBR\n",
      "413M\t../sonda/dados_formatados/TMA\n",
      "365M\t../sonda/dados_formatados/MCL\n",
      "349M\t../sonda/dados_formatados/ORN\n",
      "300M\t../sonda/dados_formatados/UBE\n",
      "284M\t../sonda/dados_formatados/BJL\n",
      "175M\t../sonda/dados_formatados/TLG\n",
      "174M\t../sonda/dados_formatados/CAI\n",
      "171M\t../sonda/dados_formatados/CTB\n",
      "55M\t../sonda/dados_formatados/CBA\n",
      "196K\t../sonda/dados_formatados/TRI\n",
      "196K\t../sonda/dados_formatados/SPK\n",
      "196K\t../sonda/dados_formatados/SCR\n",
      "196K\t../sonda/dados_formatados/RLM\n",
      "196K\t../sonda/dados_formatados/OPO\n",
      "196K\t../sonda/dados_formatados/MDS\n",
      "196K\t../sonda/dados_formatados/LEB\n",
      "196K\t../sonda/dados_formatados/CTS\n",
      "196K\t../sonda/dados_formatados/CPN\n",
      "196K\t../sonda/dados_formatados/CMS\n",
      "196K\t../sonda/dados_formatados/CHP\n",
      "196K\t../sonda/dados_formatados/BAB\n"
     ]
    }
   ],
   "source": [
    "# Exibe o tamanho de cada arquivo no diret√≥rio ordenado por tamanho de forma decrescente\n",
    "!du -h --max-depth=1 {DIRETORIO} | sort -rh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem 3 tipos de dados:\n",
    "- Dados Meteorol√≥gicos\n",
    "- Dados Solarim√©tricos\n",
    "- Dados Anemometricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# listar todos os dados Meteorol√≥gicos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_metereologicos = glob.glob(DIRETORIO + \"*/Meteorologicos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_metereologicos = [arquivo for arquivo in dados_metereologicos if 'YYYY_MM' not in arquivo]\n",
    "\n",
    "# listar todos os dados de Solarim√©tricos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_solarimetricos = glob.glob(DIRETORIO + \"*/Solarimetricos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_solarimetricos = [arquivo for arquivo in dados_solarimetricos if 'YYYY_MM' not in arquivo]\n",
    "\n",
    "# listar todos os dados de Anemometricos usando o glob s√≥ para o tipo de arquivo .csv\n",
    "dados_anemometricos = glob.glob(DIRETORIO + \"*/Anemometricos/**/*.csv\", recursive=True)\n",
    "# Remove arquivos que contenham 'YYYY_MM_MD_DQC'\n",
    "dados_anemometricos = [arquivo for arquivo in dados_anemometricos if 'YYYY_MM' not in arquivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de arquivos Meteorologicos: 1036\n",
      "Quantidade de arquivos Solarimetricos: 1022\n",
      "Quantidade de arquivos Anemometricos: 0\n"
     ]
    }
   ],
   "source": [
    "# Listar a quantidade de arquivos em cada categoria\n",
    "print(f\"Quantidade de arquivos Meteorologicos: {len(dados_metereologicos)}\")\n",
    "print(f\"Quantidade de arquivos Solarimetricos: {len(dados_solarimetricos)}\")\n",
    "print(f\"Quantidade de arquivos Anemometricos: {len(dados_anemometricos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apontar o caminho das bases de dados\n",
    "BASE_METEOROLOGICA = '../sonda/dados_meteorologicos.parquet'\n",
    "BASE_SOLARIMETRICA = '../sonda/dados_solarimetricos.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar a biblioteca DuckDB para manipula√ß√£o de dados\n",
    "import duckdb\n",
    "\n",
    "# Conectar ao banco de dados DuckDB\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Erro ao processar o arquivo: ../sonda/dados_formatados/PMA/Meteorologicos/2008/PMA_2008_06_MD_formatado.csv\n",
      "   ‚û°Ô∏è Motivo: Binder Error: table meteorological has 13 columns but 1 values were supplied\n",
      "‚ö†Ô∏è Erro ao processar o arquivo: ../sonda/dados_formatados/SLZ/Meteorologicos/2015/SLZ_2015_01_MD_formatado.csv\n",
      "   ‚û°Ô∏è Motivo: Binder Error: table meteorological has 13 columns but 1 values were supplied\n"
     ]
    },
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Table \"meteorological\" does not have a column with name \"column1\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Renomear as colunas com base nos nomes do arquivo CSV\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(column_names):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Renomeia a coluna pelo nome\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE meteorological RENAME COLUMN column\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m TO \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Salvar os dados em Parquet na BASE_METEOROLOGICA\u001b[39;00m\n\u001b[1;32m     42\u001b[0m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOPY meteorological TO \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_METEOROLOGICA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (FORMAT \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mBinderException\u001b[0m: Binder Error: Table \"meteorological\" does not have a column with name \"column1\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Verifica se o arquivo existe, caso exista leia o arquivo usando duckdb\n",
    "if  os.path.exists(BASE_METEOROLOGICA):\n",
    "    # Ler o arquivo parquet parquet usando duckdb\n",
    "    con.execute(f\"CREATE TABLE meteorological AS SELECT * FROM parquet_scan('{BASE_METEOROLOGICA}')\")\n",
    "    print(\"Tabela meteorological criada com sucesso\")\n",
    "else:\n",
    "    # Criar a tabela tempor√°ria vazia com base no primeiro arquivo CSV\n",
    "    query = f\"\"\"\n",
    "    CREATE TABLE meteorological AS \n",
    "    SELECT * FROM read_csv_auto('{dados_metereologicos[0]}', \n",
    "                                skip=2, \n",
    "                                union_by_name=True, \n",
    "                                all_varchar=True) \n",
    "    WHERE 1=0\n",
    "    \"\"\"\n",
    "    con.execute(query)  # Cria a tabela vazia com o esquema correto\n",
    "    # Inserir os dados em lote sem carregar tudo na mem√≥ria\n",
    "    for arquivo in sorted(dados_metereologicos):\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            INSERT INTO meteorological \n",
    "            SELECT * FROM read_csv_auto('{arquivo}', skip=2, union_by_name=True, all_varchar=True)\n",
    "            \"\"\"\n",
    "            con.execute(query)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao processar o arquivo: {arquivo}\")\n",
    "            print(f\"   ‚û°Ô∏è Motivo: {e}\")\n",
    "\n",
    "    # Ler a primeira linha do arquivo CSV para obter os nomes das colunas\n",
    "    header = pd.read_csv(dados_metereologicos[0], nrows=1)\n",
    "    column_names = header.columns.tolist()  # Nomes das colunas\n",
    "\n",
    "    # Renomear as colunas com base nos nomes do arquivo CSV\n",
    "    for i, col in enumerate(column_names):\n",
    "        # Formata o nome da coluna conforme a nomenclatura do DuckDB (column01, column02, ...)\n",
    "        column_name = f\"column{str(i).zfill(2)}\"  # Preenche com zero √† esquerda para 2 d√≠gitos\n",
    "        # Renomeia a coluna pelo nome correto\n",
    "        con.execute(f\"ALTER TABLE meteorological RENAME COLUMN {column_name} TO {col}\")\n",
    "\n",
    "    # Salvar os dados em Parquet na BASE_METEOROLOGICA\n",
    "    con.execute(f\"COPY meteorological TO '{BASE_METEOROLOGICA}' (FORMAT 'parquet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column00</th>\n",
       "      <th>column01</th>\n",
       "      <th>column02</th>\n",
       "      <th>column03</th>\n",
       "      <th>column04</th>\n",
       "      <th>column05</th>\n",
       "      <th>column06</th>\n",
       "      <th>column07</th>\n",
       "      <th>column08</th>\n",
       "      <th>column09</th>\n",
       "      <th>column10</th>\n",
       "      <th>column11</th>\n",
       "      <th>column12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:00:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:10:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>10</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:20:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>20</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:30:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>30</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BJL</td>\n",
       "      <td>2014-06-01 00:40:00</td>\n",
       "      <td>2014</td>\n",
       "      <td>152</td>\n",
       "      <td>40</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>3333.0</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "      <td>-5555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column00             column01 column02 column03 column04 column05 column06  \\\n",
       "0      BJL  2014-06-01 00:00:00     2014      152        0   3333.0   3333.0   \n",
       "1      BJL  2014-06-01 00:10:00     2014      152       10   3333.0   3333.0   \n",
       "2      BJL  2014-06-01 00:20:00     2014      152       20   3333.0   3333.0   \n",
       "3      BJL  2014-06-01 00:30:00     2014      152       30   3333.0   3333.0   \n",
       "4      BJL  2014-06-01 00:40:00     2014      152       40   3333.0   3333.0   \n",
       "\n",
       "  column07 column08 column09 column10 column11 column12  \n",
       "0   3333.0    -5555    -5555    -5555    -5555    -5555  \n",
       "1   3333.0    -5555    -5555    -5555    -5555    -5555  \n",
       "2   3333.0    -5555    -5555    -5555    -5555    -5555  \n",
       "3   3333.0    -5555    -5555    -5555    -5555    -5555  \n",
       "4   3333.0    -5555    -5555    -5555    -5555    -5555  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exibir as primeiras linhas da tabela meteorological\n",
    "con.execute(\"SELECT * FROM meteorological LIMIT 5\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se o arquivo '../sonda/dados_meteorologicos.parquet' existe, caso exista leia usando duckdb\n",
    "import os\n",
    "import duckdb\n",
    "\n",
    "# Verifica se o arquivo Parquet j√° existe\n",
    "if os.path.exists('../sonda/dados_meteorologicos.parquet'):\n",
    "    conn = duckdb.connect(database=':memory:', read_only=True)\n",
    "    meteorologico_parquet = conn.table('dados_meteorologicos.parquet')\n",
    "    schema_parquet = meteorologico_parquet.schema()\n",
    "    print(f\"Schema do arquivo parquet: {schema_parquet}\")\n",
    "\n",
    "else:\n",
    "    # Conectar ao banco de dados DuckDB\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # Definir schema manualmente (exemplo)\n",
    "    schema_definido = \"\"\"\n",
    "    acronym TEXT, timestamp TEXT, year INTEGER, day INTEGER, min INTEGER, \n",
    "    tp_sfc FLOAT, humid_sfc FLOAT, press FLOAT, rain FLOAT, \n",
    "    ws10_avg FLOAT, ws10_std FLOAT, wd10_avg FLOAT, wd10_std FLOAT\n",
    "    \"\"\"\n",
    "\n",
    "    # Criar tabela com schema fixo\n",
    "    con.execute(f\"CREATE TABLE meteorological ({schema_definido})\")\n",
    "\n",
    "    # Inserir os dados de forma incremental\n",
    "    for arquivo in sorted(dados_metereologicos):\n",
    "        query = f\"\"\"\n",
    "        INSERT INTO meteorological \n",
    "        SELECT \n",
    "            acronym::TEXT, timestamp::TEXT, year::INTEGER, day::INTEGER, min::INTEGER, \n",
    "            tp_sfc::FLOAT, humid_sfc::FLOAT, press::FLOAT, rain::FLOAT, \n",
    "            ws10_avg::FLOAT, ws10_std::FLOAT, wd10_avg::FLOAT, wd10_std::FLOAT\n",
    "        FROM read_csv_auto('{arquivo}', skip=2, union_by_name=True, auto_type=False)\n",
    "        \"\"\"\n",
    "        con.execute(query)\n",
    "\n",
    "    # Salvar como Parquet\n",
    "    con.execute(\"COPY meteorological TO '../sonda/dados_meteorologicos.parquet' (FORMAT 'parquet')\")\n",
    "\n",
    "    # Opcional: Verificar schema do Parquet\n",
    "    meteorologico_parquet = con.table('meteorological')\n",
    "    print(meteorologico_parquet.limit(5).df())  # Mostra 5 primeiras linhas para checar\n",
    "\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorologico_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local onde salvar o arquivo Parquet\n",
    "arquivo_parquet = '../sonda/dados_meteorologicos.parquet'\n",
    "\n",
    "# Conectar ao banco de dados DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Criar uma tabela tempor√°ria para armazenar os dados\n",
    "con.execute(\"CREATE TABLE meteorological AS SELECT * FROM read_csv_auto('\" + dados_metereologicos[0] + \"', skip=2, union_by_name=True) WHERE 1=0\")\n",
    "\n",
    "# Inserir os dados de forma incremental, sem carregar tudo na mem√≥ria\n",
    "for arquivo in sorted(dados_metereologicos):  # Pode usar toda a lista, removi o slice [0:10] para mais arquivos\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO meteorological \n",
    "    SELECT * FROM read_csv_auto('{arquivo}', skip=2, union_by_name=True)\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "\n",
    "# Pega o header do primeiro arquivo\n",
    "header = pd.read_csv(dados_metereologicos[0], nrows=1)\n",
    "\n",
    "# Salvar o resultado final em um arquivo Parquet sem carregar tudo na mem√≥ria\n",
    "con.execute(\"COPY (SELECT * FROM meteorological) TO 'dados_meteorologicos.parquet' (FORMAT 'parquet')\")\n",
    "\n",
    "# Fecha a conex√£o com DuckDB\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteorologico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para leitura r√°pida de arquivos\n",
    "def ler_csv_rapido(file, schema):\n",
    "    return pl.scan_csv(\n",
    "        file, \n",
    "        skip_rows_after_header=1,  # Pula a linha logo ap√≥s o cabe√ßalho\n",
    "        schema_overrides=schema,  # Utiliza o schema do primeiro arquivo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Verifica se arquivo j√° existe em ../sonda/ caso sim, leia o arquivo, caso n√£o, leia o arqui\n",
    "if os.path.exists('../sonda/meteorologicos.parquet'):\n",
    "    df_meteorologico = pl.read_parquet('../sonda/meteorologicos.parquet')\n",
    "else:\n",
    "    # L√™ todos os arquivos de dados meteorol√≥gicos\n",
    "    df_meteorologico = pl.concat([ler_csv_rapido(file, schema_meteorologico) for file in dados_metereologicos])\n",
    "    # Materializar os dados na mem√≥ria\n",
    "    df_meteorologico = df_meteorologico.collect()\n",
    "    # Salva os dados em um arquivo parquet\n",
    "    df_meteorologico.write_parquet('../sonda/meteorologicos.parquet', use_pyarrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para os dados solarimetricos como s√£o maiores, divida em partes\n",
    "NUM_PARTES = 30\n",
    "\n",
    "# Verifica se arquivo j√° existe em ../sonda/ caso sim, leia o arquivo, caso n√£o, leia o arqui\n",
    "if os.path.exists('../sonda/solarimetricos.parquet'):\n",
    "    df_solarimetrico = pl.read_parquet('../sonda/solarimetricos.parquet')\n",
    "else:\n",
    "    # Fun√ß√£o para unir os arquivos Parquet em partes, processando de forma incremental\n",
    "    def unir_parquets_incremental(arquivos_parquet, arquivo_saida, batch_size=5):\n",
    "        # Inicializa uma lista para os frames lidos, come√ßando com o primeiro arquivo\n",
    "        df_parcial = None\n",
    "        \n",
    "        # Processa os arquivos em lotes\n",
    "        for i in range(0, len(arquivos_parquet), batch_size):\n",
    "            # L√™ os arquivos do lote atual\n",
    "            batch_files = arquivos_parquet[i:i+batch_size]\n",
    "            lazy_frames = [pl.scan_parquet(file) for file in batch_files]\n",
    "            \n",
    "            # Concatena os arquivos no lote\n",
    "            df_batch = pl.concat(lazy_frames)\n",
    "            \n",
    "            # Materializa o DataFrame antes de adicionar ao DataFrame parcial\n",
    "            df_batch = df_batch.collect()\n",
    "\n",
    "            # Se for o primeiro lote, inicializa o DataFrame final\n",
    "            if df_parcial is None:\n",
    "                df_parcial = df_batch\n",
    "            else:\n",
    "                # Caso contr√°rio, concatena o lote no DataFrame final\n",
    "                df_parcial = df_parcial.extend(df_batch)\n",
    "            \n",
    "            # Materializa o DataFrame final antes de escrever\n",
    "            df_parcial.write_parquet(arquivo_saida, use_pyarrow=True)\n",
    "\n",
    "    # # Gerar os arquivos Parquet em partes\n",
    "    # for i in range(NUM_PARTES):\n",
    "    #     # L√™ todos os arquivos de dados solarimetricos para a parte i\n",
    "    #     df_solar = pl.concat([ler_csv_rapido(file, schema_solarimetrico) for file in dados_solarimetricos[i::NUM_PARTES]])\n",
    "    #     # Materializar os dados na mem√≥ria\n",
    "    #     df_solar = df_solar.collect()\n",
    "    #     # Salva os dados em um arquivo parquet\n",
    "    #     df_solar.write_parquet(f'../sonda/solarimetricos_{i}.parquet', use_pyarrow=True)\n",
    "\n",
    "    # Agora, unir todos os arquivos Parquet gerados\n",
    "    arquivos_parquet = [f'../sonda/solarimetricos_{i}.parquet' for i in range(NUM_PARTES)]\n",
    "\n",
    "    # Define o caminho do arquivo final\n",
    "    arquivo_saida = '../sonda/solarimetricos.parquet'\n",
    "\n",
    "    # Unir os arquivos em partes, processando de forma incremental\n",
    "    unir_parquets_incremental(arquivos_parquet, arquivo_saida, batch_size=5)\n",
    "    \n",
    "    # Remove os arquivos tempor√°rios\n",
    "    for arquivo in arquivos_parquet:\n",
    "        os.remove(arquivo)\n",
    "\n",
    "    # L√™ o arquivo final\n",
    "    df_solarimetrico = pl.read_parquet(arquivo_saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solarimetrico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
